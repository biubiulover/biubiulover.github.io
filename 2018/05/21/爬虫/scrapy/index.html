<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/100x100.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/32x32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/16x16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="python scrapy," />





  <link rel="alternate" href="/atom.xml" title="BiuBiu" type="application/atom+xml" />






<meta name="description" content="scrapy 是一个用 python 语言编写的，为了爬取网站数据，提取结构性数据而编写的应用框架。 环境本文使用的环境：python 3.5.2pip 9.0.1操作系统： Ubuntu 16.04   pythton 环境搭建在官网下载 Ubuntu 环境下的python3.5的安装包，安装完成后，检查一下 python 的安装情况，一般pyhton安装的时候，pip 也是一起安装好的，如果没">
<meta name="keywords" content="python scrapy">
<meta property="og:type" content="article">
<meta property="og:title" content="scrapy入门">
<meta property="og:url" content="http://gnbyj.cn/2018/05/21/爬虫/scrapy/index.html">
<meta property="og:site_name" content="BiuBiu">
<meta property="og:description" content="scrapy 是一个用 python 语言编写的，为了爬取网站数据，提取结构性数据而编写的应用框架。 环境本文使用的环境：python 3.5.2pip 9.0.1操作系统： Ubuntu 16.04   pythton 环境搭建在官网下载 Ubuntu 环境下的python3.5的安装包，安装完成后，检查一下 python 的安装情况，一般pyhton安装的时候，pip 也是一起安装好的，如果没">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://img-blog.csdn.net/20170709211559805?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20170709211703946?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20170709211738506?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20170709212022073?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20170709212057635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20170709212114364?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20170709212131471?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20170709212315810?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:updated_time" content="2018-05-27T15:27:00.726Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="scrapy入门">
<meta name="twitter:description" content="scrapy 是一个用 python 语言编写的，为了爬取网站数据，提取结构性数据而编写的应用框架。 环境本文使用的环境：python 3.5.2pip 9.0.1操作系统： Ubuntu 16.04   pythton 环境搭建在官网下载 Ubuntu 环境下的python3.5的安装包，安装完成后，检查一下 python 的安装情况，一般pyhton安装的时候，pip 也是一起安装好的，如果没">
<meta name="twitter:image" content="https://img-blog.csdn.net/20170709211559805?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://gnbyj.cn/2018/05/21/爬虫/scrapy/"/>





  <title>scrapy入门 | BiuBiu</title>
  








  <style>
    .forkme{
      display: none;
    }
    @media (min-width: 768px) {
      .forkme{
        display: inline;
      }
    }
  </style>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <div class="forkme">
      <a href="https://github.com/biubiulover">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_gray_6d6d6d.png" alt="Fork me on GitHub">
      </a>
    </div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BiuBiu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">GOOD IS GOOD, BUT BETTER CARRIES IT.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://gnbyj.cn/2018/05/21/爬虫/scrapy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Gavin Gao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BiuBiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">scrapy入门</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-21T18:00:00+08:00">
                2018-05-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/scrapy/" itemprop="url" rel="index">
                    <span itemprop="name">scrapy</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>scrapy 是一个用 python 语言编写的，为了爬取网站数据，提取结构性数据而编写的应用框架。</p>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>本文使用的环境：<br>python 3.5.2<br>pip 9.0.1<br>操作系统： Ubuntu 16.04  </p>
<p><strong>pythton 环境搭建</strong><br>在官网下载 Ubuntu 环境下的python3.5的安装包，安装完成后，检查一下 python 的安装情况，一般pyhton安装的时候，pip 也是一起安装好的，如果没有安装完全，再将 pip 也一起安装好。</p>
<p><strong>虚拟环境搭建</strong><br>现在Ubuntu默认是安装 python2.7 的，避免两个环境之间切换的麻烦，我们安装 python 虚拟环境来解决这个问题。</p>
<pre><code>pip install virtualenv
pip install virtualwrapper
pip list # 查看已安装
</code></pre><p>virtualenv 能够通过根据不同的 python 版本创建对应不同版本的虚拟环境，virtualwrapper 能够方便的在不同的虚拟环境之间进行切换。安装完成之后，下面我们创建一个 python3.5.2 版本的虚拟环境：</p>
<pre><code>source /usr/local/bin/virtualwrapper.sh #这个与 windows 不一样，需要先执行一下脚本才能生效，大家可以打开这个文件看一下
# 创建一个名为 py3Scrapy 的虚拟环境
mkvirtualenv py3Scrapy -p /usr/bin/python3.5
# workon 查看创建好的虚拟环境，虚拟环境的保存路径可以通过 `VIRTUALENV_PYTHON` 来配置
workon
workon py3Scrapy # 进入选择的虚拟环境
</code></pre><p>如下图所示：<br><img src="https://img-blog.csdn.net/20170709211559805?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>python的版本也能查看得到，进入虚拟环境之后，在shell前面会出现虚拟环境的名称，退出虚拟环境</p>
<pre><code>deactivate
</code></pre><p>好了，创建好环境之后，现在来开始我们的 scrapy 之旅吧。</p>
<p><strong>scrapy 环境搭建</strong><br>scrapy 是基于 twisted 框架的，大家会发现，安装 scrapy 的时候，会需要安装很多包。</p>
<pre><code>pip install scrapy
</code></pre><p>使用 pip 进行安装，方便，但是这种默认的安装方式，实在官网下载安装包来进行安装的，比较慢，大家可以使用豆瓣源来进行安装：</p>
<pre><code>pip install scrapy -i https://pypi.douban.com/simple
</code></pre><p>这种方式，下载会非常的快，安装其他的包都可以使用这种方法，但是，如果使用豆瓣源安装的时候，提示找不到符合版本的安装包，那就使用第一种办法进行下载安装，因为豆瓣源可能没有官网那么及早更新。</p>
<p>因为每个人的环境都可能存在差异，安装过程中会出现一些问题。当如果报错，twisted 安装失败的时候，建议从官网下载 twisted 安装包，自行进行安装，安装完成之后，再接着继续上面 scrapy 的安装，安装完成之后，检查一些安装结果：</p>
<pre><code>scrapy -h
</code></pre><p><strong>使用 scrapy 获取某一篇文章的信息</strong><br>好了，环境准备好之后，接下来我们来分析一下伯乐在线的文章网页结构</p>
<p><strong>分析伯乐在线某一篇文章的网页结构和url</strong><br>伯乐在线网站不需要我们先登录，然后才能访问其中的内容，所以不需要先模拟登录，直接就能访问网页。伯乐在线地址为 <a href="https://www.jobbole.com" target="_blank" rel="noopener">https://www.jobbole.com</a>，这上面的文章质量还是不错的，大家有时间可以看看。</p>
<p>我们随便找一篇文章试图来分析一下，比如 <a href="http://blog.jobbole.com/111469/" target="_blank" rel="noopener">http://blog.jobbole.com/111469/</a>，F12进入浏览器调试窗口，从全文分析，比如我们想获取文章标题，文章内容，文章创建时间，点赞数，评论数，收藏数，文章所属类别标签，文章出处等信息。</p>
<p><strong>使用 scrapy shell 的方法获取解析网页数据</strong><br>打开文章链接，我们获取到的是一个html页面，那么如何获取上面所说的那些数据呢，本文通过 CSS 选择器来获取(不了解 CSS selector的小伙伴可以先去熟悉一下 <a href="http://www.w3school.com.cn/cssref/css_selectors.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/cssref/css_selectors.asp</a>)。 scrapy 为我们提供了一个 shell 的环境，可以方便我们进行调试和实验，验证我们的css 表达式能够成功获取所需要的值。下面启动 scrapy shell</p>
<pre><code>scrapy shell &quot;http://blog.jobbole.com/111469/&quot;
</code></pre><p>scrapy 将会帮助我们将<a href="http://blog.jobbole.com/111469/这个链接的数据捕获，现在来获取一下文章标题，在浏览器中找到文章标题，inspect" target="_blank" rel="noopener">http://blog.jobbole.com/111469/这个链接的数据捕获，现在来获取一下文章标题，在浏览器中找到文章标题，inspect</a> element 审查元素，如下图所示：<br><img src="https://img-blog.csdn.net/20170709211703946?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br>文章标题为王垠：如何掌握所有的程序语言，从上图获知，这个位于一个 class 名为 entry-header 的 div 标签下的子标签 h1 中，那我们在 scrapy shell 通过 css 选择器来获取一下，如下图所示：<br><img src="https://img-blog.csdn.net/20170709211738506?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br>仔细查看上图，注意一些细节。通过 response.css 方法，返回的结果是一个 selector，不是字符串，在这个 selector 的基础上可以继续使用 css 选择器。通过 extract() 函数获取提取的标题内容，返回结果是一个 list，注意，这里是一个 list ，仍然不是字符串 str，使用 extract()[0] 返回列表中的第一个元素，即我们需要的标题。</p>
<p>但是，如果标题没有获取到，或者选择器返回的结果为空的话，使用 extract()[0] 就会出错，因为试图对一个空链表进行访问，这里使用 extract_first() 方法更加合适，可是使用一个默认值，当返回结果为空的时候，返回这个默认值</p>
<pre><code>extract_first(&quot;&quot;)   # 默认值为 &quot;&quot;
</code></pre><p>此处仅仅是将 title 标题作为一个例子进行说明，其他的就不详细进行解释了，主要代码如下所示：</p>
<pre><code>title = response.css(&quot;.entry-header h1::text&quot;).extract()[0]
match_date = re.match(&quot;([0-9/]*).*&quot;,
                      response.css(&quot;.entry-meta-hide-on-mobile::text&quot;).extract()[0].strip())
if match_date:
    create_date = match_date.group(1)

votes_css = response.css(&quot;.vote-post-up h10::text&quot;).extract_first()
if votes_css:
    vote_nums = int(votes_css)
else:
    vote_nums = 0

ma_fav_css = re.match(&quot;.*?(\d+).*&quot;,
                      response.css(&quot;.bookmark-btn::text&quot;).extract_first())
if ma_fav_css:
    fav_nums = int(ma_fav_css.group(1))
else:
    fav_nums = 0

ma_comments_css = re.match(&quot;.*?(\d+).*&quot;,
                           response.css(&quot;a[href=&apos;#article-comment&apos;] span::text&quot;).extract_first())
if ma_comments_css:
    comment_nums = int(ma_comments_css.group(1))
else:
    comment_nums = 0

tag_lists_css = response.css(&quot;.entry-meta-hide-on-mobile a::text&quot;).extract()
tag_lists_css = [ele for ele in tag_lists_css if not ele.strip().endswith(&apos;评论&apos;)]
tags = &apos;,&apos;.join(tag_lists_css)

content = response.css(&quot;.entry *::text&quot;).extract()
</code></pre><p>解释一下 create_date，通过获取到的值，存在其他非时间的数据，通过 re.match 使用正则表达式来提取时间。</p>
<p>好了，所有需要的值都提取成功后，下面通过 scrapy 框架来创建我们的爬虫项目。</p>
<p><strong>创建爬虫项目</strong><br>开始我们的爬虫项目</p>
<pre><code>scrapy startproject ArticleSpider
</code></pre><p>scrapy 会为我们创建一个名为 ArticleSpider 的项目<br>进入到 ArticleSpider 目录，使用basic模板创建</p>
<pre><code>scrapy genspider jobbole blog.jobbole.com
</code></pre><p>创建完成之后，我们使用 pycharm 这个IDE打开我们创建的爬虫项目，目录结构如下所示： </p>
<pre><code>├── ArticleSpider
│   ├── items.py
│   ├── middlewares.py
│   ├── pipelines.py
│   ├── __pycache__
│   ├── settings.py
│   ├── spiders
│   │   ├── __init__.py
│   │   ├── jobbole.py
│   │   └── __pycache__
└── scrapy.cfg
</code></pre><p>我们可以在 items.py 里面定义数据保存的格式，在 middlewares.py 定义中间件，在 piplines.py 里面处理数据，保存到文件或者数据库中等。在 jobbole.py 中对爬取的页面进行解析。</p>
<p>下面，我们首先需要做的，就是利用我们编写的 css 表达式，获取我们提取的文章的值。在 jobbole.py 中，我们看到</p>
<pre><code>class JobboleSpider(scrapy.Spider):
    name = &apos;jobbole&apos;
    allowed_domains = [&apos;blog.jobbole.com&apos;]
    start_urls = [&apos;http://blog.jobbole.com/all-posts/&apos;]

    def parse(self, response):
        pass
</code></pre><p>scrapy 为我们创建了一个 JobboleSpider 的类，name 是爬虫项目的名称，同时定义了域名以及爬取的入口链接。scrapy 初始化的时候，会初始化 <code>start_urls</code> 入口链接列表，然后通过 <code>start_requests</code> 返回 Request 对象进行下载，调用 parse 回调函数对页面进行解析，提取需要的值，返回 item。</p>
<p>所以，我们需要做的，就是将我们在上一小节编写的代码放在 parse 函数中，同时，将 <code>start_urls</code> 的值，改为上面我们在 scrapy shell 爬取的页面的地址<a href="http://blog.jobbole.com/111469/" target="_blank" rel="noopener">http://blog.jobbole.com/111469/</a>，因为我们这里还没有讲到通过 item 获取我们提取的值，此处你可以通过 print() 函数将值进行打印。在 shell 中启动爬虫（先进入我们的工程目录）</p>
<pre><code>scrapy crawl jobbole
</code></pre><p>既然我们使用了 pycharm 这个IDE，那么我们就不用 shell 来启动爬虫，在 ArticleSpider 目录下创建一个 main.py 文件</p>
<pre><code>from scrapy.cmdline import execute
import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;jobbole&quot;])
</code></pre><p>上面的代码，就是将当前项目路径加入到 path 中，然后通过调用scrapy 命令行来启动我们的工程。然后，通过设置断点调试，一步一步查看我们的提取的变量的值是否正确。</p>
<blockquote>
<p>注意：启动之前，将 settings.py 中的 <code>ROBOTSTXT_OBEY</code> 这个参数设置为 False</p>
</blockquote>
<p>这样，我们就爬取到了伯乐在线的这一篇文章了。</p>
<p><strong>扩展，爬取所有的文章</strong><br>既然我们已经能够获取到某一篇文章的数据，那么下面就来获取所有文章的链接。</p>
<p><strong>扩展一：获取所有 url 链接</strong><br>伯乐在线所有文章链接的入口地址为 <a href="http://blog.jobbole.com/all-posts/" target="_blank" rel="noopener">http://blog.jobbole.com/all-posts/</a>，通过浏览器进入调试模式查看文章列表的链接，如下图所示<br><img src="https://img-blog.csdn.net/20170709212022073?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br>文章链接是在 id 为 archive 的 div 标签下的子 div 标签之下， class 为 post-thumb，这个下面的子标签 a 的 href 属性，仍使用上面说的 scrapy shell 的方法，如下图所示<br><img src="https://img-blog.csdn.net/20170709212057635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br>可以看出，获得了当前页面所有的文章的 url，这仅仅是当前页面的所有 url，我们还需要获取下一页的 url，然后通过下一页的 url 进入到下一页，获取下一页的所有文章的 url，依次类推，知道爬取完所有的文章 url。</p>
<p>在文章列表的最后，有翻页，分析如下<br><img src="https://img-blog.csdn.net/20170709212114364?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br>下一页是 class 为 next page-numbers 的 a 标签中，如下图<br><img src="https://img-blog.csdn.net/20170709212131471?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br>既然现在所有的 url 都能够获取到了，那么现在我们将 jobbole.py 中的 parse 函数修改一下</p>
<pre><code>def parse(self, response):
    post_nodes = response.css(&quot;#archive .floated-thumb .post-thumb&quot;)    # a selector, 可以在这个基础上继续做 selector

    for post_node in post_nodes:
        post_url = post_node.css(&quot;a::attr(href)&quot;).extract_first(&quot;&quot;)
        yield Request(url=parse.urljoin(response.url, post_url),
                      callback=self.parse_detail)

    # 必须考虑到有前一页，当前页和下一页链接的影响，使用如下所示的方法
    next_url = response.css(&quot;span.page-numbers.current+a::attr(href)&quot;).extract_first(&quot;&quot;)
     if next_url:
         yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse)

def parse_detail(self, response):
    &quot;&quot;&quot;作为回调函数，在上面调用&quot;&quot;&quot;
    title = response.css(&quot;.entry-header h1::text&quot;).extract()[0]
    match_date = re.match(&quot;([0-9/]*).*&quot;,
                          response.css(&quot;.entry-meta-hide-on-mobile::text&quot;).extract()[0].strip())
    if match_date:
        create_date = match_date.group(1)

    votes_css = response.css(&quot;.vote-post-up h10::text&quot;).extract_first()
    if votes_css:
        vote_nums = int(votes_css)
    else:
        vote_nums = 0

    ma_fav_css = re.match(&quot;.*?(\d+).*&quot;,
                          response.css(&quot;.bookmark-btn::text&quot;).extract_first())
    if ma_fav_css:
        fav_nums = int(ma_fav_css.group(1))
    else:
        fav_nums = 0

    ma_comments_css = re.match(&quot;.*?(\d+).*&quot;,
                               response.css(&quot;a[href=&apos;#article-comment&apos;] span::text&quot;).extract_first())
    if ma_comments_css:
        comment_nums = int(ma_comments_css.group(1))
    else:
        comment_nums = 0

    tag_lists_css = response.css(&quot;.entry-meta-hide-on-mobile a::text&quot;).extract()
    tag_lists_css = [ele for ele in tag_lists_css if not ele.strip().endswith(&apos;评论&apos;)]
    tags = &apos;,&apos;.join(tag_lists_css)

    # cpyrights = response.css(&quot;.copyright-area&quot;).extract()
    content = response.css(&quot;.entry *::text&quot;).extract()
</code></pre><ol>
<li>获取文章列表页中的文章url，交给 scrapy 下载后并进行解析，即调用 parse 函数解析 </li>
<li>然后获取下一页的文章 url，按照1 2 循环</li>
</ol>
<p>对于 parse 函数，一般做三种事情:<br>a. 解析返回的数据 response data<br>b. 提取数据，生成 ITEM<br>c. 生成需要进一步处理 URL 的 Request 对象 </p>
<blockquote>
<p>某些网站中，url 仅仅只是一个后缀，需要将当前页面的url+后缀进行拼接，使用的是 parse.urljoin(base, url)，如果 urljoin 中的 url 没有域名，将使用base进行拼接，如果有域名，将不会进行拼接,此函数在 python3 的 urllib 库中。Request(meta参数)：meta参数是一个字典{},作为回调函数的参数</p>
</blockquote>
<p>这样，我们就获得了所有的文章</p>
<p><strong>扩展二：使用item，并保存图片到本地</strong><br>上一小节提到了， parse 函数提取数据之后，生成 item，scrapy 会通过 http 将 item 传到 pipeline 进行处理，那么这一小节，我们使用 item 来接收 parse 提取的数据。在 items.py 文件中，定义一个我们自己的数据类 JobBoleArticleItem，并继承 scrapy.item 类</p>
<pre><code>class JobBoleArticleItem(scrapy.Item):
    title = scrapy.Field()          # Field()能够接收和传递任何类型的值,类似于字典的形式
    create_date = scrapy.Field()    # 创建时间
    url = scrapy.Field()            # 文章路径
    front_img_url_download = scrapy.Field()
    fav_nums = scrapy.Field()       # 收藏数
    comment_nums = scrapy.Field()   # 评论数
    vote_nums = scrapy.Field()      # 点赞数
    tags = scrapy.Field()           # 标签分类 label
    content = scrapy.Field()        # 文章内容
    object_id = scrapy.Field()      # 文章内容的md5的哈希值，能够将长度不定的 url 转换成定长的序列
</code></pre><p>Field() 对象，能够接收和传递任何类型的值，看源代码，就能发现，Field() 类继承自 dict 对象，具有字典的所有属性。</p>
<p>注意，在上面定义的类中，我们增加了一个新的成员变量 <code>front_img_url_download</code>，这是保存的是文章列表中，每一个文章的图片链接。我们需要将这个图片下载到本地环境中。既然使用了 item 接收我们提取的数据，那么 parse 函数就需要做相应的改动</p>
<pre><code>def parse(self, response):
    post_nodes = response.css(&quot;#archive .floated-thumb .post-thumb&quot;)    # a selector, 可以在这个基础上继续做 selector

    for post_node in post_nodes:
        post_url = post_node.css(&quot;a::attr(href)&quot;).extract_first(&quot;&quot;)
        img_url = post_node.css(&quot;a img::attr(src)&quot;).extract_first(&quot;&quot;)
        yield Request(url=parse.urljoin(response.url, post_url),
                      meta={&quot;front-image-url&quot;:img_url}, callback=self.parse_detail)

    # 必须考虑到有前一页，当前页和下一页链接的影响，使用如下所示的方法
    next_url = response.css(&quot;span.page-numbers.current+a::attr(href)&quot;).extract_first(&quot;&quot;)
     if next_url:
         yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse)
</code></pre><p>同时，解析函数 <code>parse_detail</code> 也需要修改，将数据保存到我们的item中，只需要添加下面的部分就可</p>
<pre><code>front_img_url = response.meta.get(&quot;front-image-url&quot;, &quot;&quot;)
article_item = JobBoleArticleItem() # 实例化 item 对象
# 赋值 item 对象
article_item[&quot;title&quot;] = title
article_item[&quot;create_date&quot;] = create_date
article_item[&quot;url&quot;] = response.url
article_item[&quot;front_img_url_download&quot;] = [front_img_url] # 这里传递的需要是列表的形式，否则后面保存图片的时候，会出现类型错误，必须是可迭代对象
article_item[&quot;fav_nums&quot;] = fav_nums
article_item[&quot;comment_nums&quot;] = comment_nums
article_item[&quot;vote_nums&quot;] = vote_nums
article_item[&quot;tags&quot;] = tags
# article_item[&quot;cpyrights&quot;] = cpyrights
article_item[&quot;content&quot;] = &apos;&apos;.join(content)      # 取出的 content 是一个 list ,存入数据库的时候，需要转换成字符串
article_item[&quot;object_id&quot;] = gen_md5(response.url)
yield article_item
</code></pre><p>这里，parse 函数成功生成了我们定义的 item 对象，将数据传递到 pipeline。那么，图片链接已经获取到了，我们如下将图片下载下来呢。</p>
<blockquote>
<p>解释一下上面代码中的 front-img-url，这个是在 parse 函数中作为参数 meta 传递给 Request() 函数，回调函数调用 <code>parse_detail</code>，返回的 response 对象中的 meta 成员，将包含这个元素， meta 就是一个字典， <code>response.meta.get(&quot;front-image-url&quot;)</code> 将获取到我们传递过来的图片url</p>
</blockquote>
<p>scrapy 提供了一个 ImagesPipeline 类，可直接用于图片操作，只需要我们在 settings.py 文件中进行配置即可。</p>
<p>在 settings.py 中，有一个配置参数为 <code>ITEM_PIPELINE</code>，这其实就是一个字典，当需要用到 pipeline 时，就需要在这个字典中进行配置，字典中存在的， scrapy 才会使用。字典中的 key 就是 pipeline 的类名，后面的数字表示优先级，数字越小表示越先调用，越大越靠后。既然我们现在需要使用到 scrapy 提供的图片下载功能，那么需要在这个字典中配置 ImagesPipeline</p>
<pre><code>ITEM_PIPELINES = {
   &apos;scrapy.pipelines.images.ImagesPipeline&apos;: 1,
}
</code></pre><p>同时，还需要在 settings.py 中配置，item 中哪一个字段是图片 url，以及图片需要存放什么位置</p>
<pre><code>IMAGES_URLS_FIELD = &quot;front_img_url_download&quot; # ITEM 中的图片 URL，用于下载
PROJECT_IMAGE_PATH = os.path.abspath(os.path.dirname(__file__))   # 获取当前文件所在目录
IMAGES_STORE = os.path.join(PROJECT_IMAGE_PATH, &quot;images&quot;) # 下载图片的保存位置
</code></pre><p>这些参数，可以在 ImagesPipeline 类的源代码中查看到</p>
<blockquote>
<p>注意：上面配置好后，上面的代码是在工程路径下面创建一个 images 的目录，用于保存图片，运行 main.py，可能会出现如下错误: no module named PIL，这是因为图片操作需要 pillow 库，只需要安装即可<br>pip install pillow，快速安装，就按照我上面说的豆瓣源的方法。<br>还可能出现”ValueError: Missing scheme in request url: h”的错误，这是因为图片操作，要求 <code>front_img_url_download</code> 的值为 list 或者可以迭代的对象，所以我们在 parse 函数中给 item 赋值的时候， <code>front_img_url_download</code> 就是赋值的 list 对象</p>
</blockquote>
<p>好了，这些注意了之后，应该能够下载图片了。</p>
<p><strong>扩展三：使用 itemloader</strong><br>相信大家已经发现，虽然使用了 item，但是使用 css selecotor，我们的 parse 函数显得很长，而且，当数据量越来越大之后，一大堆的 css 表达式是很难维护的。在加上正则表达式的提取，代码会显得很臃肿。这里，给大家推荐是用 itemloader。itemloader 可以看成是一个容器。</p>
<p>首先，在 items.py 中，我们需要定义一个继承自 ItemLoader 的类</p>
<pre><code>class ArticleItemLoader(ItemLoader):
    &quot;&quot;&quot;
    自定义 ItemLoader, 就相当于一个容器
    &quot;&quot;&quot;
    # 这里表示，输出获取的 ArticleItemLoader 提取到的值，都是 list 中的第一个值
    # 如果有的默认不是取第一个值，就在 Field() 中进行修改
    default_output_processor = TakeFirst()
</code></pre><p>将默认输出函数定为 <code>TakeFirst()</code>，即取结果 list 中的第一个值，定义了 ItemLoader 类之后，需要修改 jobbole.py 中的 <code>parse_detail</code> 函数了，现在就不再直接使用 css selector 了，使用 itemloader 中的 css 进行数据提取，新的 <code>parse_detail</code> 如下所示：</p>
<pre><code>def parse_detail(self, response):
    front_img_url = response.meta.get(&quot;front-image-url&quot;, &quot;&quot;)
    item_loader = ArticleItemLoader(item=JobBoleArticleItem(), response=response)
    article_item_loader = JobBoleArticleItem()
    item_loader.add_css(&quot;title&quot;, &quot;.entry-header h1::text&quot;)  # 通过 css 选择器获取值
    item_loader.add_value(&quot;url&quot;, response.url)
    item_loader.add_css(&quot;create_date&quot;, &quot;.entry-meta-hide-on-mobile::text&quot;)
    item_loader.add_value(&quot;front_img_url_download&quot;, [front_img_url])
    item_loader.add_css(&quot;fav_nums&quot;, &quot;.bookmark-btn::text&quot;)
    item_loader.add_css(&quot;comment_nums&quot;, &quot;a[href=&apos;#article-comment&apos;] span::text&quot;)
    item_loader.add_css(&quot;vote_nums&quot;, &quot;.vote-post-up h10::text&quot;)
    item_loader.add_css(&quot;tags&quot;, &quot;.entry-meta-hide-on-mobile a::text&quot;)
    item_loader.add_css(&quot;content&quot;, &quot;.entry *::text&quot;)
    item_loader.add_value(&quot;object_id&quot;, gen_md5(response.url))
    # item_loader.add_xpath()
    # item_loader.add_value()
    article_item_loader = item_loader.load_item()
    yield article_item_loader
</code></pre><p>这样，数据提取就全部交给了 <code>item_loader</code> 来执行了。代码整体都简洁和工整了很多。ItemLoader 有三个方法用于提取数据，分别是 <code>add_css()</code>, <code>add_xpath()</code>, <code>add_value()</code>，前两个分别是 css 选择器和 xpath 选择器，如果是值，就直接使用 <code>add_value()</code> 即可。</p>
<p>最后 <code>load_item()</code> 函数，将根据上面提供的规则进行数据解析，每一个解析的值都是以 list 结果的形式呈现，同时，将结果赋值 item。</p>
<p>但是，大家应该已经发现，之前我们直接使用 css selector 提取数据的时候，对于某些数据，需要使用正则表达式进行匹配才能获取所需的值，这里什么都没做，仅仅是通过 itemloader 提取了数据而已。所以，我们还需要重新定义我们的 item 类，这些操作在 item 中进行处理。修改 items.py 中的 JobBoleArticleItem 类，具体如下：</p>
<pre><code>class JobBoleArticleItem(scrapy.item):
    title = scrapy.Field()
    create_date = scrapy.Field(     # 创建时间
        input_processor = MapCompose(get_date),
        output_processor = Join(&quot;&quot;)
    )
    url = scrapy.Field()            # 文章路径
    front_img_url_download = scrapy.Field(    # 文章封面图片路径,用于下载，赋值时必须为数组形式
        # 默认 output_processor 是 TakeFirst()，这样返回的是一个字符串，不是 list，此处必须是 list
        # 修改 output_processor
        output_processor = MapCompose(return_value)
    )
    front_img_url = scrapy.Field()
    fav_nums = scrapy.Field(        # 收藏数
        input_processor=MapCompose(get_nums)
    )
    comment_nums = scrapy.Field(    # 评论数
        input_processor=MapCompose(get_nums)
    )
    vote_nums = scrapy.Field(       # 点赞数
        input_processor=MapCompose(get_nums)
    )
    tags = scrapy.Field(           # 标签分类 label
        # 本身就是一个list, 输出时，将 list 以 commas 逗号连接
        input_processor = MapCompose(remove_comment_tag),
        output_processor = Join(&quot;,&quot;)
    )
    content = scrapy.Field(        # 文章内容
        # content 我们不是取最后一个，是全部都要，所以不用 TakeFirst()
        output_processor=Join(&quot;&quot;)
    )
    object_id = scrapy.Field()      # 文章内容的md5的哈希值，能够将长度不定的 url 转换成定长的序列
</code></pre><p><code>input_processor</code> 对传入的值进行预处理， <code>output_processor</code> 对处理后的值按照规则进行处理和提取，比如 TakeFirst() 就是对处理的结果取第一个值。</p>
<p><code>input_processor = MapCompose(func1, func2, func3, ...)</code> 这行代码，说明的是， Item 传入的这个字段的值，将会分别调用 MapCompose 中的所有传入的方法进行逐个处理，这个方法也是可以是 lambda 的匿名函数。</p>
<p>因为上面定义 ArticleItemLoader 类的时候，使用了默认的 <code>default_output_processor</code>，如果不想使用默认的这个方法，就在 Field() 中，使用 <code>output_processor</code> 参数覆盖默认的方法，哪怕什么都不做，也不会使用默认方法获取数据了。对上面那些方法定义如下：</p>
<pre><code>def get_nums(value):
    &quot;&quot;&quot;
    通过正则表达式获取 评论数，点赞数和收藏数
    &quot;&quot;&quot;
    re_match = re.match(&quot;.*?(\d+).*&quot;, value)
    if re_match:
        nums = (int)(re_match.group(1))
    else:
        nums = 0

    return nums


def get_date(value):
    re_match = re.match(&quot;([0-9/]*).*?&quot;, value.strip())
    if re_match:
        create_date = re_match.group(1)
    else:
        create_date = &quot;&quot;
    return create_date

def remove_comment_tag(value):
    &quot;&quot;&quot;
    去掉 tag 中的 “评论” 标签
    &quot;&quot;&quot;
    if &quot;评论&quot; in value:
        return &quot;&quot;
    else:
        return value


def return_value(value):
    &quot;&quot;&quot;
    do nothing, 只是为了覆盖 ItemLoader 中的 default_processor
    &quot;&quot;&quot;
    return value
</code></pre><blockquote>
<p>千万注意：这些方法，每一个最后，都必须有 return，否则程序到后面将获取不到这个字段的数据，再次访问这个字段的时候，就会报错。</p>
</blockquote>
<p><strong>扩展四：将数据导出到 json 文件中</strong><br>好了，既然已经将数据通过 ItemLoader 获取到了，那么我们现在就将数据从 pipeline 输出到 json 文件中。<br>将数据以 json 格式输出，可以通过 json 库的方法，也可以使用 scrapy.exporters 的方法。</p>
<p><strong>json 库</strong><br>我们已经知道，对数据的处理，scrapy 是在 pipeline 中进行的，所以，我们需要在 pipelines.py 中定义我们对数据的导出操作。创建一个新类</p>
<pre><code>class JsonWithEncodingPipeline(object):
    &quot;&quot;&quot;
    处理 item 数据，保存为json格式的文件中
    &quot;&quot;&quot;
    def __init__(self):
        self.file = codecs.open(&apos;article.json&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;)

    def process_item(self, item, spider):
        lines = json.dumps(dict(item), ensure_ascii=False) + &apos;\n&apos;   # False，才能够在处理非acsii编码的时候，不会出错，尤其
        #中文
        self.file.write(lines)
        return item     # 必须 return

    def spider_close(self, spider):
        &quot;&quot;&quot;
        把文件关闭
        &quot;&quot;&quot;
        self.file.close()
</code></pre><p><code>__init__()</code> 构造对象的时候，就打开文件，scrapy 会调用 process_item() 函数对数据进行处理，在这个函数中，将数据以 json 的格式写入文件中。操作完成之后，将文件关闭。思路很简单。</p>
<p><strong>scrapy.exporters 的方式</strong>  </p>
<pre><code>class JsonExporterPipeline(object):
    def __init__(self):
        &quot;&quot;&quot;
        先打开文件，传递一个文件
        &quot;&quot;&quot;
        self.file = open(&apos;articleexporter.json&apos;, &apos;wb&apos;)
        #调用 scrapy 提供的 JsonItemExporter导出json文件
        self.exporter = JsonItemExporter(self.file, encoding=&quot;utf-8&quot;, ensure_ascii=False)
        self.exporter.start_exporting()

    def spider_close(self, spider):
        self.exporter.finish_exporting()
        self.file.close()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item
</code></pre><p>scrapy.exporters 提供了几种不同格式的文件支持，能够将数据输出到这些不同格式的文件中，查看 JsonItemExporter 源码即可获知</p>
<pre><code>__all__ = [&apos;BaseItemExporter&apos;, &apos;PprintItemExporter&apos;, &apos;PickleItemExporter&apos;,
           &apos;CsvItemExporter&apos;, &apos;XmlItemExporter&apos;, &apos;JsonLinesItemExporter&apos;,
           &apos;JsonItemExporter&apos;, &apos;MarshalItemExporter&apos;]
</code></pre><p>这些就是 scrapy 支持的文件。方法名称都差不多，这算是 scrapy 运行 pipeline 的模式，只需要将逻辑处理放在 process_item()，scrapy 就会根据规则对数据进行处理。</p>
<p>当然，要想使我们写的数据操作有效，别忘记了，在 settings.py 中进行配置</p>
<pre><code>ITEM_PIPELINES = {

  &apos;scrapy.pipelines.images.ImagesPipeline&apos;: 1,
  &apos;ArticleSpider.pipelines.JsonWithEncodingPipeline&apos;: 2,
  &apos;ArticleSpider.pipelines.JsonExporterPipeline&apos;:3,
}
</code></pre><p><strong>扩展五：将数据存储到 MySQL 数据库</strong><br>前面介绍了将数据以 json 格式导出到文件，那么将数据保存到 MySQL 中，如何操作，相信大家已经差不多了然于胸了。这里也介绍两种方法，一种是通过 MySQLdb 的API来实现的数据库存取操作，这种方法简单，适合用与数据量不大的场合，如果数据量大，数据库操作的速度跟不上数据解析的速度，就会造成数据拥堵。那么使用第二种方法就更好，使用 twisted 框架提供的异步操作方法，不会造成拥堵，速度更快。</p>
<p>既然是入 MySQL 数据库，首先肯定是需要创建数据库表了。表结构如下图所示：<br><img src="https://img-blog.csdn.net/20170709212315810?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaG9uZ2xpY3UxMjM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br>上图中有一个字段的值，我没有讲述怎么取，就是 <code>front_img_path</code> 这个值，大家在数据库入库的时候，直接用空置或者空字符串填充即可。这个字段是保存图片在本地保存的路径，这个需要在 ImagesPipe 的 <code>item_completed(self, results, item, info)</code> 方法中的 results 参数中获取。</p>
<p>好了，数据库表创建成功之后，下面就来将数据入库了。</p>
<p><strong>MySQLdb 的方法入库</strong>  </p>
<pre><code>class MysqlPipeline(object):
    def __init__(self):
        # 连接数据库
        self.conn = MySQLdb.connect(&apos;192.168.0.101&apos;, &apos;spider&apos;, &apos;wuzhenyu&apos;, &apos;article_spider&apos;, charset=&quot;utf8&quot;, use_unicode=True)
        self.cursor = self.conn.cursor()

    def process_item(self, item, spider):
        insert_sql = &quot;&quot;&quot;
            insert into article(title, create_date, url, url_object_id, front_img_url, front_img_path, comment_nums, 
            fav_nums, vote_nums, tags, content) VALUES (&apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, %d, %d, %d, &apos;%s&apos;, &apos;%s&apos;)
        &quot;&quot;&quot; % (item[&quot;title&quot;], item[&quot;create_date&quot;], item[&quot;url&quot;], item[&quot;object_id&quot;],item[&quot;front_img_url&quot;],
               item[&quot;front_img_path&quot;], item[&quot;comment_nums&quot;], item[&quot;fav_nums&quot;], item[&quot;vote_nums&quot;], item[&quot;tags&quot;],
               item[&quot;content&quot;])

        self.cursor.execute(insert_sql)
        self.conn.commit()

    def spider_close(self, spider):
        self.cursor.close()
        self.conn.close()
</code></pre><p>如果对 API 想了解的更多，就去阅读 python MySQLdb 的相关API文档说明，当然，要想这个生效，首先得在 settings.py 文件中将这个 pipeline 类加入 ITEM_PIPELINE 字典中</p>
<pre><code>ITEM_PIPELINES = {

  &apos;scrapy.pipelines.images.ImagesPipeline&apos;: 1,
  &apos;ArticleSpider.pipelines.JsonWithEncodingPipeline&apos;: 2,
  &apos;ArticleSpider.pipelines.JsonExporterPipeline&apos;:3,
  &apos;ArticleSpider.pipelines.MysqlPipeline&apos;: 4,
}
</code></pre><p><strong>通过 Twisted 框架提供的异步方法入库</strong>  </p>
<pre><code>class MysqlTwistedPipeline(object):
    &quot;&quot;&quot;
    利用 Twisted API 实现异步入库 MySQL 的功能
    Twisted 提供的是一个异步的容器，MySQL 的操作还是使用的MySQLDB 的库
    &quot;&quot;&quot;
    def __init__(self, dbpool):
        self.dbpool = dbpool

    @classmethod
    def from_settings(cls, settings):
        &quot;&quot;&quot;
        被 spider 调用，将 settings.py 传递进来，读取我们配置的参数
        模仿 images.py 源代码中的 from_settings 函数的写法
        &quot;&quot;&quot;
        # 字典中的参数，要与 MySQLdb 中的connect 的参数相同
        dbparams = dict(
            host = settings[&quot;MYSQL_HOST&quot;],
            db = settings[&quot;MYSQL_DBNAME&quot;],
            user = settings[&quot;MYSQL_USER&quot;],
            passwd = settings[&quot;MYSQL_PASSWORD&quot;],
            charset = &quot;utf8&quot;,
            cursorclass = MySQLdb.cursors.DictCursor,
            use_unicode = True
        )

        # twisted 中的 adbapi 能够将sql操作转变成异步操作
        dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, **dbparams)
        return cls(dbpool)

    def process_item(self, item, spider):
        &quot;&quot;&quot;
        使用 twisted 将 mysql 操作编程异步执行
        &quot;&quot;&quot;
        query = self.dbpool.runInteraction(self.do_insert, item)
        query.addErrback(self.handle_error) # handle exceptions

    def handle_error(self, failure):
        &quot;&quot;&quot;
        处理异步操作的异常
        &quot;&quot;&quot;
        print(failure)

    def do_insert(self, cursor, item):
        &quot;&quot;&quot;
        执行具体的操作，能够自动 commit
        &quot;&quot;&quot;
        print(item[&quot;create_date&quot;])
        insert_sql = &quot;&quot;&quot;
                    insert into article(title, create_date, url, url_object_id, front_img_url, front_img_path, comment_nums, 
                    fav_nums, vote_nums, tags, content) VALUES (&apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, %d, %d, %d, &apos;%s&apos;, &apos;%s&apos;);
                &quot;&quot;&quot; % (item[&quot;title&quot;], item[&quot;create_date&quot;], item[&quot;url&quot;], item[&quot;object_id&quot;], item[&quot;front_img_url&quot;],
                       item[&quot;front_img_path&quot;], item[&quot;comment_nums&quot;], item[&quot;fav_nums&quot;], item[&quot;vote_nums&quot;], item[&quot;tags&quot;],
                       item[&quot;content&quot;])

        # self.cursor.execute(insert_sql, (item[&quot;title&quot;], item[&quot;create_date&quot;], item[&quot;url&quot;], item[&quot;object_id&quot;],
        #                                 item[&quot;front_img_url&quot;], item[&quot;front_img_path&quot;], item[&quot;comment_nums&quot;],
        #                                 item[&quot;fav_nums&quot;], item[&quot;vote_nums&quot;], item[&quot;tags&quot;], item[&quot;content&quot;]))
        print(insert_sql)
        cursor.execute(insert_sql)
</code></pre><p>需要提到的是，上面定义的 <code>from_settings(cls. settings)</code> 这个类方法， scrapy 会从 settings.py<br>文件中读取配置进行加载，这里将 MySQL 的一些配置信息放在了 settings.py 文件中，然后使用 <code>from_settings</code> 方法直接获取，在 settings.py 中需要添加如下代码：</p>
<pre><code># MySQL params
MYSQL_HOST = &quot;&quot;
MYSQL_DBNAME = &quot;article_spider&quot;
MYSQL_USER = &quot;spider&quot;
MYSQL_PASSWORD = &quot;&quot;
</code></pre><p>本篇文章，主要以 scrapy 框架爬取伯乐在线文章为例，简要介绍了 scrapy 爬取数据的一些方法，博主也是最近才开始学习爬虫，有不对的地方还请大家能够指正。</p>
<p>windows 中安装环境与 Ubuntu 会有一些不一样，而且如果使用的是 python3.x 版本，会要求 vc++ 的版本比较高，最好安装的是 visual studio 2015 以上的版本。否则会很麻烦。</p>
<p>对于 python2.7版本，在windows中，可以安装 VSForPython27.msi ，依赖的那些库应该就不会再出错了。</p>

      
    </div>
    
    
    

    
      <div>
        <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/images/kev168.jpg" alt="Gavin Gao wechat" style="width: 200px; max-width: 100%;"/>
    <div>欢迎您扫一扫上面的二维码添加好友，一起探讨&进步！</div>
</div>

      </div>
    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持好文分享，您的支持将鼓励我继续进步！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Gavin Gao 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Gavin Gao 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python-scrapy/" rel="tag"># python scrapy</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/15/家装/家装注意点/" rel="next" title="装修最容易犯的错误有哪些？">
                <i class="fa fa-chevron-left"></i> 装修最容易犯的错误有哪些？
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/05/29/程序问题/Error notes/" rel="prev" title="在错误中成长">
                在错误中成长 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Gavin Gao" />
            
              <p class="site-author-name" itemprop="name">Gavin Gao</p>
              <p class="site-description motion-element" itemprop="description">也许就在这一瞬间 你的笑容一人如晚霞般 在川流不息的时光中 神采飞扬</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/biubiulover" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:gnkevin@126.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#环境"><span class="nav-number">1.</span> <span class="nav-text">环境</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Gavin Gao</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a></div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
